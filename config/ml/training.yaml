# training.yaml - Configuración avanzada de entrenamiento para agentes PPO
# Define modos, balances, hitos, simulación y hiperparámetros
# Usado por core/ml/reinforcement_agent.py y core/ml/autolearn.py
# Versión: 1.0.0

version: "1.0.0"

training:
  balance:                    # Balance inicial y objetivo por símbolo
    initial: 10000.0         # Balance inicial (rango: 1000.0-100000.0 USDT)
    target: 15000.0          # Balance objetivo (rango: 1.1-2.0 * initial)
  modes:
    historical:               # Entrenamiento con datos históricos
      enabled: true
      lookback_days: 365      # 1 año de datos (rango: 180-730)
      timesteps: 100000       # Total timesteps (rango: 50000-500000)
    realtime:                 # Entrenamiento en tiempo real
      enabled: true
      lookback_days: 30       # 30 días de histórico (rango: 7-90)
      timesteps: 50000        # Timesteps por reentrenamiento (rango: 10000-100000)
  simulation:                 # Parámetros de simulación
    slippage: 0.001         # Slippage simulado (rango: 0.0-0.005)
    fees: 0.0006            # Fee por trade (rango: 0.0002-0.001, basado en BitGet)
  objectives:                 # Objetivos de desempeño
    min_sharpe: 1.5         # Reentrenar si Sharpe < 1.5 (rango: 1.0-3.0)
    max_drawdown: 0.05      # Detener si drawdown >5% (rango: 0.03-0.10)
    min_trades_per_day: 5   # Mínimo trades diarios (rango: 3-10)
    min_win_rate: 0.60      # Mínimo win rate diario (rango: 0.50-0.80)
  milestones:                 # Recompensas por hitos (sincronizadas con rewards.yaml)
    balance_10pct: 50.0     # +10% del balance inicial (rango: 20.0-100.0)
    balance_20pct: 100.0    # +20% del balance inicial (rango: 50.0-200.0)
    sharpe_above_2: 30.0    # Sharpe >2.0 en 24h (rango: 10.0-50.0)
    win_rate_above_60: 20.0 # Win rate >60% en 24h (rango: 10.0-40.0)
    consecutive_wins:       # Trades exitosos consecutivos
      threshold: 5          # Mínimo 5 trades (rango: 3-10)
      reward: 25.0          # Recompensa (rango: 10.0-50.0)
  overfitting_control:         # Control de overfitting
    max_policy_entropy: 0.5  # Entropía máxima (rango: 0.2-1.0)
    early_stopping:          # Detener si no mejora
      patience: 10000        # Timesteps sin mejora (rango: 5000-20000)
      min_improvement: 0.01  # Mínima mejora en recompensa (rango: 0.005-0.05)
  hyperparameters:            # Hiperparámetros PPO
    learning_rate: 0.0003    # Tasa de aprendizaje (rango: 0.0001-0.001)
    batch_size: 64           # Tamaño de batch (rango: 32-256)
    n_steps: 2048            # Pasos por actualización (rango: 512-4096)
    gamma: 0.99              # Factor de descuento (rango: 0.9-0.999)
    ent_coef: 0.01           # Coeficiente de entropía (rango: 0.0-0.1)
